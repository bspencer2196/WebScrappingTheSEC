{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79229bba",
   "metadata": {},
   "source": [
    "# Import the modules you will need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9621906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules need for working with date and time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "\n",
    "# Modules need for webscraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# modules needed for working with DataFrames and Excel\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Needed to remove warnings from jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62ce8e",
   "metadata": {},
   "source": [
    "# Change User-Agent in requests headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58415968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the \"User-Agent\" for you header otherwise the sec will reject your\n",
    "# requests and you will get a 403 error\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({\"User-Agent\": \"insert name here, enter email\",})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9b21e",
   "metadata": {},
   "source": [
    "# Put in the inputs of what you are looking for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47b0574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What companies cik number are you looking for: input ticker AAPL\n",
      "How many years of filings do you want? 2\n",
      "Are you looking for 10-K, or 10-Q filings? 10-K\n"
     ]
    }
   ],
   "source": [
    "# input the companies ticker and whether you want their 10Ks or 10Qs\n",
    "search_for_ticker = input(\"What companies cik number are you looking for: input ticker \")\n",
    "\n",
    "# maybe put in what time frame you are looking for\n",
    "search_through_files_years_back = int(input('How many years of filings do you want? '))\n",
    "#ten_ks_or_qs = input('Do you want the Ks or the Qs') #impliment later\n",
    "\n",
    "type_of_filing = input('Are you looking for 10-K, or 10-Q filings? ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a8ec8",
   "metadata": {},
   "source": [
    "# Set the dates you are grabbing files for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb52ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = date.today()\n",
    "till_date = todays_date - relativedelta(years=search_through_files_years_back)\n",
    "todays_date = str(todays_date).replace('-','/')\n",
    "till_date = str(till_date).replace('-','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29451df0",
   "metadata": {},
   "source": [
    "# Send the SEC a Request for all their company tickers and CIK numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d11611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is the url where all the sec tickers are found\n",
    "url_of_tickers = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "# send SEC the page request\n",
    "page = requests.get(url_of_tickers, headers = headers)\n",
    "json_ticker_dictionary = page.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619ab72",
   "metadata": {},
   "source": [
    "# Put the Data into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793f120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with your ticker dictionary\n",
    "df = pd.DataFrame(json_ticker_dictionary)\n",
    "\n",
    "# transpose the data frame so it is easier to view\n",
    "df = df.transpose()\n",
    "\n",
    "# set the index to the ticker column that way it's easy to search for\n",
    "# and return the cik number\n",
    "df.set_index(keys = 'ticker', inplace = True)\n",
    "\n",
    "# search for the ticker and store the cik in a var\n",
    "cik_num = str(df.loc[search_for_ticker, 'cik_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f41376",
   "metadata": {},
   "source": [
    "# Send the SEC a Request to get data on all the company filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f338f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary string because the SEC has the company files.json down\n",
    "url_of_company_filings = f\"https://www.sec.gov/Archives/edgar/data/320193/index.json\"\n",
    "\n",
    "# I don't know why i set this request up different but it works\n",
    "page = requests.get(url_of_company_filings, headers = headers).text\n",
    "accession_number = json.loads(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd8c45",
   "metadata": {},
   "source": [
    "# Create a DataFrame with all the company filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe27040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the dictionary of 'items' aka the filings into a DataFrame\n",
    "df = pd.DataFrame(accession_number['directory']['item'])\n",
    "\n",
    "# create a column with of the url of every indiviudal filing folder \n",
    "df[\"Url\"] = f\"https://www.sec.gov/Archives/edgar/data/{cik_num}/\" + df[\"name\"] + \"/index.json\"\n",
    "\n",
    "# create a temp url\n",
    "base_url = f\"https://www.sec.gov/Archives/edgar/data/{cik_num}/\"\n",
    "#df['Url'] = \"https://www.sec.gov/Archives/edgar/data/320193/\" + df['name'] + '/index.json'\n",
    "\n",
    "# eleminate columns that aren't needed. This is an unnecessary step\n",
    "df.drop([\"type\",\"size\"], axis= \"columns\", inplace= True)\n",
    "\n",
    "# rename the last-modified column to date\n",
    "df.rename(columns={\"last-modified\":\"Date\", \"name\":\"Accession #\"}, inplace=True)\n",
    "\n",
    "# split the date and time into seperate columns and remove the time column\n",
    "df[['Date', 'Time']] = df['Date'].str.split(\" \", n=1, expand = True)\n",
    "df.drop(['Time'], axis=\"columns\", inplace=True) # if programs get more advanced you can bring back the time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d493d8f",
   "metadata": {},
   "source": [
    "# Filter the DataFrame to return the filings between a given dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97b1a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put the Date column into datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# only return the filings urls between cetain dates\n",
    "mask = (df['Date'] <= todays_date) & (df['Date'] >= till_date)\n",
    "df = df[mask]\n",
    "\n",
    "# create an integer index of data frame and remove the index column\n",
    "df.reset_index(inplace= True)\n",
    "df.drop('index', axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f6423",
   "metadata": {},
   "source": [
    "# Loop through the DataFrame, create a Dataframe to return the '.txt' filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157d91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store store the filings that are 10q, 10k or 8k etc.\n",
    "# It is in the filings fold which is the url created in the DataFrame above.\n",
    "# Which will be put into a data frame later.\n",
    "filings_list = []\n",
    "\n",
    "# loop through the filing folders\n",
    "for i in range(len(df.index)):\n",
    "    \n",
    "    # store the filings fold url\n",
    "    filings_folder_url = df.iloc[i,2]\n",
    "    \n",
    "    # request the SEC for filing folders page and store data into dictionary\n",
    "    page = requests.get(filings_folder_url, headers=headers).text\n",
    "    folder_filing_dictionary = json.loads(page)\n",
    "    \n",
    "    # create a DataFrame with the filiing folder information\n",
    "    filing_df = pd.DataFrame(folder_filing_dictionary['directory']['item'])\n",
    "    \n",
    "    # return the filing ending with '.txt'\n",
    "    filings_list.append(filing_df.iloc[2])\n",
    "\n",
    "    # the sec can only accept 10 request a second so sleep it for .10 or .11 secs\n",
    "    time.sleep(.11)\n",
    "    \n",
    "    # the code below is to see if you're getting the '.txt' filing\n",
    "#    print(filing_df.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153df6f",
   "metadata": {},
   "source": [
    "# Create a DataFrame to store all the filings with '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6392a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame named fdf aka filings DataFrame\n",
    "fdf = pd.DataFrame(filings_list, columns=[\"last-modified\", \"name\", \"type\", \"size\"])\n",
    "\n",
    "# reset your index\n",
    "fdf = fdf.reset_index(level=0)\n",
    "\n",
    "# remove all the uneccessary columns\n",
    "fdf.drop(['last-modified', 'type', 'size', 'index'],axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7519bd4",
   "metadata": {},
   "source": [
    "# Merge your Filings DataFrame with your other DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "609a38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge your fdf to your df\n",
    "df = pd.merge(df, fdf, left_index= True, right_index= True)\n",
    "\n",
    "# convert the column types from objects to a string\n",
    "df['Url'] = df['Url'].astype('string')\n",
    "df['name'] = df['name'].astype('string')\n",
    "\n",
    "# remove the 'index.json'\n",
    "df['Url'] = df['Url'].str.replace('index.json', '')\n",
    "\n",
    "# create a new column of the individual filing urls ending in '.txt'\n",
    "df['Filing_urls'] = df['Url'] + df['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af54eb33",
   "metadata": {},
   "source": [
    "# Loop through DataFrame to find yearly (10ks) (pending.. or quarterly reports (10qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a43a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenk_dictionary = {}\n",
    "\n",
    "for i in range(len(df.index)):\n",
    "    # loop through your filings column send a page request to the SEC\n",
    "    url = df['Filing_urls'][i]\n",
    "    page = requests.get(url, headers = headers).text\n",
    "    doc = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    # find the filing type\n",
    "    filing_type = doc.type.find(text= True, recursive = False).strip()\n",
    "    \n",
    "    # create a dictionary with all the urls and accession numbers \n",
    "    # of your filing type you chose\n",
    "    if filing_type == type_of_filing:     \n",
    "#         print(url)\n",
    "        # THIS MIGHT BE WHERE YOU PUT THE ACCESSION NUMBER\n",
    "        tenk_dictionary[df['Date'][i]] = df['Url'][i], df['Accession #'][i]\n",
    "    \n",
    "    # SEC only accepts 10 requests a second so make sure you sleep the loop.\n",
    "    time.sleep(.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2eb26",
   "metadata": {},
   "source": [
    "# Create DataFrame with FilingSummary.xml Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc6d6ce7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tenk_dictionary.items(), columns= ['Date', 'Url'])\n",
    "\n",
    "# Clean your data\n",
    "# and add column for FilingSummary.xml\n",
    "df[['Url','Accession #']] = df['Url'].tolist()\n",
    "df['Url'] = df['Url'].astype('string')\n",
    "df['Accession #'] = df['Accession #'].astype('string')\n",
    "df['Filing.xml_url'] =  df['Url'] + 'FilingSummary.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbae5b6",
   "metadata": {},
   "source": [
    "# Heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df7e1692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribution Error: carry on!\n",
      "Attribution Error: carry on!\n"
     ]
    }
   ],
   "source": [
    "# Append to this list all to a dictionary outside the for loop\n",
    "dictionary_lists = []\n",
    "\n",
    "for i in range(len(df.index)):\n",
    "    \n",
    "    # create the basis for your dictionary\n",
    "    master_dictionary_of_all_tables = {}\n",
    "    saucey_short_name_list = []\n",
    "    saucey_html_file_name_list = []\n",
    "    \n",
    "    # loop through your filings column send a page request to the SEC\n",
    "    url = df['Filing.xml_url'][i]\n",
    "    xml_page = requests.get(url, headers = headers)\n",
    "    sauce = BeautifulSoup(xml_page.content, 'lxml')\n",
    "    bod = sauce.find('body')\n",
    "    \n",
    "    # I cannot get this to run without an Attribution error\n",
    "    # just except and move to the next thing\n",
    "    try:\n",
    "        for link in bod.find_all('report'):\n",
    "            # insert data into your lists\n",
    "            saucey_short_name_list.append(link.find('shortname').string)\n",
    "            saucey_html_file_name_list.append(link.find('htmlfilename').string)\n",
    "            \n",
    "            #(code) the code below might stop the attribution error if put here\n",
    "            \n",
    "            # Use the list above to create your dictionary\n",
    "            master_dictionary_of_all_tables[str(df['Date'][i])] = saucey_short_name_list\n",
    "            master_dictionary_of_all_tables[df['Accession #'][i]] = saucey_html_file_name_list\n",
    "            \n",
    "    except AttributeError:\n",
    "        print('Attribution Error: carry on!')\n",
    "        next\n",
    "        \n",
    "    #(code) this code might need to be placed right after the list     \n",
    "    while len(saucey_short_name_list) != len(saucey_html_file_name_list):\n",
    "        saucey_short_name_list.pop()\n",
    "    \n",
    "    # apped all your dictionaries to a list\n",
    "    dictionary_lists.append(master_dictionary_of_all_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52452802",
   "metadata": {},
   "source": [
    "# Turn the dictionary list into a DataFrame list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69e97e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list = []\n",
    "\n",
    "for i in range(len(dictionary_lists)):\n",
    "    df = pd.DataFrame(dictionary_lists[i])\n",
    "    dataframe_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1449d9",
   "metadata": {},
   "source": [
    "# Loop through DataFramesList, rename the first column to 'ShortName' and then merg all the DataFrames in the DataFramesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f5ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change first column to 'ShortName'\n",
    "for i in range(len(dataframe_list)):\n",
    "    dataframe_list[i].rename({dataframe_list[i].columns[0]:'ShortName'}, axis=1, inplace=True)\n",
    "    dataframe_list[i].set_index('ShortName', inplace=True)\n",
    "    \n",
    "# merge all DataFrames into one.\n",
    "for i in range(len(dataframe_list)-1):\n",
    "    dataframe_list[0] = dataframe_list[0].join(dataframe_list[i+1],  how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db305f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new DF that's = to the first item in dataframe list\n",
    "filing_df = dataframe_list[0]\n",
    "len_of_filing_df = len(filing_df.columns)\n",
    "\n",
    "# create column of the urls you're going to scrape tables for\n",
    "for i in range(len_of_filing_df):\n",
    "    filing_df['Url' + str(i)] =  base_url + filing_df.columns[i] + '/' + filing_df[filing_df.columns[i]]\n",
    "    \n",
    "# drop unnecessary columns    \n",
    "filing_df.drop(filing_df.columns[0:len_of_filing_df], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4fd17cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select data you want to grab, income statement, balance sheet, cash flows, etc.\n",
    "filing_df = filing_df.iloc[[1,3,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3f8b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list will hold all the data frames that you will be joining\n",
    "list_page_lists_requests = []\n",
    "\n",
    "#df_list = []\n",
    "for x in range(len(filing_df.index)):\n",
    "    page_list_requests = []\n",
    "    for i in filing_df.columns:\n",
    "        \n",
    "        # requests data\n",
    "        page = requests.get(filing_df[i][x], headers = headers).text\n",
    "        # put into a list\n",
    "        page_list_requests.append(page)\n",
    "    # put the list above into a list every individual list will\n",
    "    # be put into a DataFrame\n",
    "    list_page_lists_requests.append(page_list_requests)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80665761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all DataFrames into this list\n",
    "lists_of_df_lists = []\n",
    "\n",
    "for x in range(len(list_page_lists_requests)):\n",
    "    df_list = []\n",
    "    \n",
    "    for i in list_page_lists_requests[x]:\n",
    "        \n",
    "        # read data into a DataFrame\n",
    "        df = pd.read_html(i)\n",
    "        df = df[0]\n",
    "        \n",
    "        # if DF is a multileve index remove it\n",
    "        if isinstance(df.columns, pd.core.indexes.multi.MultiIndex):\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "        \n",
    "        # rename lineitems column to ledger and set it as the index \n",
    "        df.rename({df.columns[0]:'Ledger'}, axis=1, inplace= True)\n",
    "        df.set_index('Ledger', inplace= True)\n",
    "        \n",
    "        # set df to string to remove unwanted characters\n",
    "        df = df.astype('string')\n",
    "        \n",
    "        # clean data so you can convert DF astype to float\n",
    "        for i in df.columns.to_list():\n",
    "            df[i] = df[i].str.replace(',','')\n",
    "            df[i] = df[i].str.replace('$','')\n",
    "            df[i] = df[i].str.replace(')','')\n",
    "            df[i] = df[i].str.replace('(','-')\n",
    "\n",
    "        df = df.astype('float')\n",
    "        \n",
    "        # make sure index has unique values otherwise\n",
    "        # they will not join properly it adds numbers to any\n",
    "        # duplicate line items\n",
    "        df_ledger = df.index.to_list()\n",
    "        \n",
    "        UniqueValues = set(df_ledger)\n",
    "\n",
    "        for y in UniqueValues:\n",
    "            number = 0\n",
    "            \n",
    "            for z in range(0,len(df_ledger)):\n",
    "                if df_ledger[z] == y:\n",
    "                    number += 1\n",
    "                    if number >= 2:\n",
    "                        df_ledger[z] += str(number)\n",
    "                              \n",
    "        # set the new index with no duplicate items                \n",
    "        df.set_index(pd.Index(df_ledger), drop=True, inplace=True)\n",
    "    \n",
    "    # append df to df_list and df_list to your lists of dfs list\n",
    "        df_list.append(df)\n",
    "    lists_of_df_lists.append(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eee24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list will hold your merged data frames that will be exported to excel\n",
    "list_of_merged_dfs = []\n",
    "\n",
    "# this goes thought you lists of df list grabs every individual df_list\n",
    "# inside and merges them all to gether.\n",
    "for x in range(len(lists_of_df_lists)):\n",
    "    df_static = lists_of_df_lists[x][0]\n",
    "\n",
    "    for i in range(len(lists_of_df_lists[x])-1):\n",
    "        temp_df1 = lists_of_df_lists[x][i]\n",
    "        temp_df2 = lists_of_df_lists[x][i+1]\n",
    "\n",
    "        cols_to_use = temp_df2.columns.difference(temp_df1.columns)\n",
    "\n",
    "        temp_df2 = temp_df2[cols_to_use]  \n",
    "\n",
    "        df_static = df_static.join(temp_df2,  how='left', rsuffix='_y')\n",
    "    \n",
    "    list_of_merged_dfs.append(df_static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5822edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for nameing your excel sheets\n",
    "list_of_financials = ['IncomeStatement','BalanceSheet','CashFlows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0b66f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where you would like to save your file\n",
    "save_to_path = r\"C:\\Users\\{insert Username}\\OneDrive\\Desktop\"\n",
    "\n",
    "# specify file name\n",
    "excel_file_name = r'\\{insert file name}.xlsx'\n",
    "\n",
    "# create full path\n",
    "full_path = save_to_path + excel_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bed7f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all dfs into excel\n",
    "list_of_merged_dfs[0].to_excel(full_path, sheet_name=list_of_financials[0])\n",
    "\n",
    "book = load_workbook(full_path)\n",
    "writer = pd.ExcelWriter(full_path, engine='openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "list_of_merged_dfs[1].to_excel(writer, sheet_name=list_of_financials[1])\n",
    "list_of_merged_dfs[2].to_excel(writer, sheet_name=list_of_financials[2])\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
